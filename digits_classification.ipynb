{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digits_classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QXPAbFwqf2Rd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST digits classification with TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "zZu1Lyb3f2Rg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/hse-aml/intro-to-dl/blob/master/week2/v2/images/mnist_sample.png?raw=1\" style=\"width:30%\">"
      ]
    },
    {
      "metadata": {
        "id": "3EdHT2z2f2Rh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5ee16521-8432-424b-8b0b-1c31bac1628e"
      },
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "setup_google_colab.setup_week2_v2()  # change to the week you're working on\n",
        "# note on week 2: select setup_week2_v2() if you've started the course after August 13, 2018,\n",
        "# otherwise call setup_week2().\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import grading\n",
        "\n",
        "import matplotlib_utils\n",
        "from importlib import reload\n",
        "reload(matplotlib_utils)\n",
        "\n",
        "import grading_utils\n",
        "reload(grading_utils)\n",
        "\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n",
            "**************************************************\n",
            "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "**************************************************\n",
            "cifar-10-batches-py.tar.gz\n",
            "**************************************************\n",
            "mnist.npz\n",
            "We're using TF 1.11.0-rc2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hllXYx4zf2Rr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Look at the data\n",
        "\n",
        "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
        "We will train a classifier on this data."
      ]
    },
    {
      "metadata": {
        "id": "B3M37CcVf2Rs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import preprocessed_mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5vTB0Vlnf2Rv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "4f320d4f-a53b-45d5-f04a-abf40ceecf6a"
      },
      "cell_type": "code",
      "source": [
        "# X contains rgb values divided by 255\n",
        "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
        "print(\"A closeup of a sample patch:\")\n",
        "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"And the whole sample:\")\n",
        "plt.imshow(X_train[1], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train [shape (50000, 28, 28)] sample patch:\n",
            " [[0.         0.29803922 0.96470588 0.98823529 0.43921569]\n",
            " [0.         0.33333333 0.98823529 0.90196078 0.09803922]\n",
            " [0.         0.33333333 0.98823529 0.8745098  0.        ]\n",
            " [0.         0.33333333 0.98823529 0.56862745 0.        ]\n",
            " [0.         0.3372549  0.99215686 0.88235294 0.        ]]\n",
            "A closeup of a sample patch:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPQAAAD4CAYAAADb7cuFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyP8AFG4GWya6GjoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe184083b00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the whole sample:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnzPOFW1kYgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe183fef470>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "y_train [shape (50000,)] 10 samples:\n",
            " [5 0 4 1 9 2 1 3 1 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qq1AKzTtf2Ry",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear model\n",
        "\n",
        "Your task is to train a linear classifier $\\vec{x} \\rightarrow y$ with SGD using TensorFlow.\n",
        "\n",
        "You will need to calculate a logit (a linear transformation) $z_k$ for each class: \n",
        "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad k = 0..9$$\n",
        "\n",
        "And transform logits $z_k$ to valid probabilities $p_k$ with softmax: \n",
        "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n",
        "\n",
        "We will use a cross-entropy loss to train our multi-class classifier:\n",
        "$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
        "\n",
        "where \n",
        "$$\n",
        "[x]=\\begin{cases}\n",
        "       1, \\quad \\text{if $x$ is true} \\\\\n",
        "       0, \\quad \\text{otherwise}\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want.\n",
        "\n",
        "Here's the plan:\n",
        "* Flatten the images (28x28 -> 784) with `X_train.reshape((X_train.shape[0], -1))` to simplify our linear model implementation\n",
        "* Use a matrix placeholder for flattened `X_train`\n",
        "* Convert `y_train` to one-hot encoded vectors that are needed for cross-entropy\n",
        "* Use a shared variable `W` for all weights (a column $\\vec{w_k}$ per class) and `b` for all biases.\n",
        "* Aim for ~0.93 validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "or-PGNGjf2Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7a01a356-18b9-404e-a4a5-079fa785d053"
      },
      "cell_type": "code",
      "source": [
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "print(X_train_flat.shape)\n",
        "\n",
        "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_val_flat.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YFZ8qopsf2R1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "49367a0f-b50f-4418-87e3-e38df9812fac"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
        "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "print(y_train_oh[:3], y_train[:3])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 10)\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKriWUuYf2R4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# run this again if you remake your graph\n",
        "s = reset_tf_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bNrkEdpif2R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model parameters: W and b\n",
        "W = tf.get_variable(\"W\", shape = (784, 10), dtype = 'float32' )### YOUR CODE HERE ### tf.get_variable(...) with shape[0] = 784\n",
        "b = tf.get_variable(\"b\", shape = (784, 10), dtype = 'float32')### YOUR CODE HERE ### tf.get_variable(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fSyzO7sSf2SN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Placeholders for the input data\n",
        "input_X = tf.placeholder(dtype = 'float32', shape = (None, 784), name = \"X\")### YOUR CODE HERE ### tf.placeholder(...) for flat X with shape[0] = None for any batch size\n",
        "input_y = tf.placeholder(dtype = 'float32', shape = (None, 10), name = \"y\")### YOUR CODE HERE ### tf.placeholder(...) for one-hot encoded true labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7STNtiUyf2SP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compute predictions\n",
        "logits = tf.matmul(input_X, W) + b### YOUR CODE HERE ### logits for input_X, resulting shape should be [input_X.shape[0], 10]\n",
        "probas = tf.nn.softmax(logits)### YOUR CODE HERE ### apply tf.nn.softmax to logits\n",
        "classes = tf.argmax(probas)### YOUR CODE HERE ### apply tf.argmax to find a class index with highest probability\n",
        "\n",
        "# Loss should be a scalar number: average loss over all the objects with tf.reduce_mean().\n",
        "# Use tf.nn.softmax_cross_entropy_with_logits on top of one-hot encoded input_y and logits.\n",
        "# It is identical to calculating cross-entropy on top of probas, but is more numerically friendly (read the docs).\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy(input_y,logits))### YOUR CODE HERE ### cross-entropy loss\n",
        "\n",
        "# Use a default tf.train.AdamOptimizer to get an SGD step\n",
        "step = ### YOUR CODE HERE ### optimizer step that minimizes the loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIqqQX3ef2SS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s.run(tf.global_variables_initializer())\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 40\n",
        "\n",
        "# for logging the progress right here in Jupyter (for those who don't have TensorBoard)\n",
        "simpleTrainingCurves = matplotlib_utils.SimpleTrainingCurves(\"cross-entropy\", \"accuracy\")\n",
        "\n",
        "for epoch in range(EPOCHS):  # we finish an epoch when we've looked at all training samples\n",
        "    \n",
        "    batch_losses = []\n",
        "    for batch_start in range(0, X_train_flat.shape[0], BATCH_SIZE):  # data is already shuffled\n",
        "        _, batch_loss = s.run([step, loss], {input_X: X_train_flat[batch_start:batch_start+BATCH_SIZE], \n",
        "                                             input_y: y_train_oh[batch_start:batch_start+BATCH_SIZE]})\n",
        "        # collect batch losses, this is almost free as we need a forward pass for backprop anyway\n",
        "        batch_losses.append(batch_loss)\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    val_loss = s.run(loss, {input_X: X_val_flat, input_y: y_val_oh})  # this part is usually small\n",
        "    train_accuracy = accuracy_score(y_train, s.run(classes, {input_X: X_train_flat}))  # this is slow and usually skipped\n",
        "    valid_accuracy = accuracy_score(y_val, s.run(classes, {input_X: X_val_flat}))  \n",
        "    simpleTrainingCurves.add(train_loss, val_loss, train_accuracy, valid_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6U0ZwuQuf2SU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submit a linear model"
      ]
    },
    {
      "metadata": {
        "id": "YCOyu_Pbf2SV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Testing shapes \n",
        "grader.set_answer(\"9XaAS\", grading_utils.get_tensors_shapes_string([W, b, input_X, input_y, logits, probas, classes]))\n",
        "# Validation loss\n",
        "grader.set_answer(\"vmogZ\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\n",
        "# Validation accuracy\n",
        "grader.set_answer(\"RMv95\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tb6jSbymf2SY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VewuPLssf2Sb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Adding more layers"
      ]
    },
    {
      "metadata": {
        "id": "nSPd4H55f2Sc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's add a couple of hidden layers and see how that improves our validation accuracy.\n",
        "\n",
        "Previously we've coded a dense layer with matrix multiplication by hand. \n",
        "But this is not convinient, you have to create a lot of variables and your code becomes a mess. \n",
        "In TensorFlow there's an easier way to make a dense layer:\n",
        "```python\n",
        "hidden1 = tf.layers.dense(inputs, 256, activation=tf.nn.sigmoid)\n",
        "```\n",
        "\n",
        "That will create all the necessary variables automatically.\n",
        "Here you can also choose an activation function (rememeber that we need it for a hidden layer!).\n",
        "\n",
        "Now add 2 hidden layers to the code above and restart training.\n",
        "You're aiming for ~0.97 validation accuracy here."
      ]
    },
    {
      "metadata": {
        "id": "_6EaN4wqf2Sc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# you can write the code here to get a new `step` operation and then run cells with training loop above\n",
        "# name your variables in the same way (e.g. logits, probas, classes, etc) for safety\n",
        "### YOUR CODE HERE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0KztqvJ4f2Sf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submit a 2-layer MLP\n",
        "Run these cells after training a 2-layer MLP"
      ]
    },
    {
      "metadata": {
        "id": "-ZFF23i4f2Sf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## GRADED PART, DO NOT CHANGE!\n",
        "# Validation loss for MLP\n",
        "grader.set_answer(\"i8bgs\", s.run(loss, {input_X: X_val_flat, input_y: y_val_oh}))\n",
        "# Validation accuracy for MLP\n",
        "grader.set_answer(\"rE763\", accuracy_score(y_val, s.run(classes, {input_X: X_val_flat})))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5sJ1TIgf2TL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# you can make submission with answers so far to check yourself at this stage\n",
        "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jjvfh-Ynf2TO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}